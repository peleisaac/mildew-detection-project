{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1c2b35-8615-4370-a30a-f6b3f89c14b9",
   "metadata": {},
   "source": [
    "## üì• Data Collection\n",
    "\n",
    "This notebook handles the collection of the raw dataset used in this project.\n",
    "\n",
    "- The dataset is hosted on Kaggle and requires authentication via the Kaggle API.\n",
    "- We configure Kaggle credentials securely and download the dataset programmatically.\n",
    "- Once downloaded, the dataset is extracted from its zip archive and made ready for further analysis.\n",
    "\n",
    "This phase does **not** involve any modification or transformation of the data. The goal is to retrieve the dataset in its original form for inspection and preprocessing.\n",
    "\n",
    "### Objective\n",
    "Fetch data from the kaggle url and prepare it for further processes\n",
    "\n",
    "### Inputs\n",
    "Kaggle JSON file - the authentication token.\n",
    "\n",
    "### Outputs\n",
    "Generate Dataset: inputs/datasets/cherry_leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18327354-bf11-4c08-a5fa-037fcd096173",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6598c5ec-25db-40fc-bf09-72b51cf9c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8706213-4b72-4522-8a37-0b997a3af8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found: ./kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        if \"kaggle.json\" in file:\n",
    "            print(\"‚úÖ Found:\", os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cce74fb-9d2b-4d09-bf4f-bd2ecbf99d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically find the kaggle.json path\n",
    "config_path = os.path.join(os.getcwd(), \"kaggle.json\")\n",
    "\n",
    "# Set Kaggle config env var\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.path.dirname(config_path)\n",
    "\n",
    "# Apply permissions\n",
    "!chmod 600 kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e459d-2d5d-4d68-ae86-287a41bd40a4",
   "metadata": {},
   "source": [
    "#### ‚ÑπÔ∏è Note:\n",
    "Downloading the dataset from kaggle website to our defined destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93852063-ca31-4c11-9825-66e7d676fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/codeinstitute/cherry-leaves\n",
      "License(s): unknown\n",
      "Downloading cherry-leaves.zip to ../inputs/cherry_leaves\n",
      "  0%|                                               | 0.00/55.0M [00:00<?, ?B/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55.0M/55.0M [00:00<00:00, 1.45GB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d \"codeinstitute/cherry-leaves\" -p \"../inputs/cherry_leaves\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c71ee-57cb-472c-abea-73870ee369fd",
   "metadata": {},
   "source": [
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4833cae-b248-4ef8-9f11-54ea2ed8c76a",
   "metadata": {},
   "source": [
    "**Unzip the downloaded files using the zipfile.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdc4a82-ea41-4366-9883-d5a99d885959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"../inputs/cherry_leaves/cherry-leaves.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../inputs/cherry_leaves/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c113b6-3e85-4a71-8499-fd79fac5350a",
   "metadata": {},
   "source": [
    "### Dataset Statistics Collection\n",
    "We count the total number of images in each class (healthy vs infected cherry leaves) and save the statistics for use in the Streamlit dashboard metrics.\n",
    "- üìå **Insight:** This provides essential dataset overview information including total size and class distribution balance for model development planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb062de-3af8-41ca-9687-e859ea8973ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset stats saved to outputs/dataset_stats.json\n",
      "Total Images: 4208\n",
      "Healthy Leaves: 2104\n",
      "Infected Leaves: 2104\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Count images in each folder\n",
    "healthy_path = \"../inputs/cherry_leaves/cherry-leaves/healthy\"\n",
    "infected_path = \"../inputs/cherry_leaves/cherry-leaves/powdery_mildew\"\n",
    "\n",
    "healthy_count = len(os.listdir(healthy_path))\n",
    "infected_count = len(os.listdir(infected_path))\n",
    "total_count = healthy_count + infected_count\n",
    "\n",
    "# Create results dictionary\n",
    "dataset_stats = {\n",
    "    \"total_images\": total_count,\n",
    "    \"healthy_leaves\": healthy_count,\n",
    "    \"powdery_mildew_leaves\": infected_count\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"../outputs/dataset_stats.json\", \"w\") as f:\n",
    "    json.dump(dataset_stats, f, indent=2)\n",
    "\n",
    "print(f\"Dataset stats saved to outputs/dataset_stats.json\")\n",
    "print(f\"Total Images: {total_count}\")\n",
    "print(f\"Healthy Leaves: {healthy_count}\")\n",
    "print(f\"Infected Leaves: {infected_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1210f8-15cf-4e9f-9198-f2d0aec56b1c",
   "metadata": {},
   "source": [
    "## Split Dataset into Train, Validation, and Test\n",
    "\n",
    "This function organizes the cleaned cherry leaf dataset into separate subsets: \n",
    "- **70% for training**, \n",
    "- **20% for validation**, and \n",
    "- **10% for testing**.\n",
    "\n",
    "We also:\n",
    "- **Skip hidden folders/files** such as `.ipynb_checkpoints` which are automatically generated by Jupyter and are not actual image data.\n",
    "- **Filter for only valid image formats** like `.jpg`, `.jpeg`, and `.png` to avoid including non-image files by mistake.\n",
    "- **Separate output directory** is used to preserve the original dataset. This approach ensures data integrity, supports reproducibility, and allows for safe reprocessing or experimentation without altering the source files.\n",
    "\n",
    "This structure supports robust training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27e5249-c477-42b9-91ea-b0e86fbba7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def organize_dataset_into_subsets(image_dir, output_dir, splits=(0.7, 0.2, 0.1)):\n",
    "    \"\"\"\n",
    "    Organizes the dataset into train, validation, and test folders.\n",
    "    Skips hidden folders and filters only valid image formats.\n",
    "    \"\"\"\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    image_dir = Path(image_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Skip hidden/system folders like .ipynb_checkpoints\n",
    "    labels = [d.name for d in image_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "    print(f\"Found labels: {labels}\")\n",
    "\n",
    "    for label in labels:\n",
    "        files = [\n",
    "            f for f in (image_dir / label).iterdir()\n",
    "            if f.is_file() and not f.name.startswith('.') and f.suffix.lower() in valid_extensions\n",
    "        ]\n",
    "        random.shuffle(files)\n",
    "\n",
    "        train_count = int(splits[0] * len(files))\n",
    "        val_count = int(splits[1] * len(files))\n",
    "\n",
    "        subsets = {\n",
    "            \"train\": files[:train_count],\n",
    "            \"validation\": files[train_count:train_count + val_count],\n",
    "            \"test\": files[train_count + val_count:]\n",
    "        }\n",
    "\n",
    "        for subset_name, subset_files in subsets.items():\n",
    "            subset_path = output_dir / subset_name / label\n",
    "            subset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for file in subset_files:\n",
    "                shutil.copy(file, subset_path)\n",
    "\n",
    "    print(\"\\n‚úÖ Dataset split complete.\")\n",
    "    for subset in [\"train\", \"validation\", \"test\"]:\n",
    "        total = sum(1 for _ in (output_dir / subset).rglob(\"*.*\"))\n",
    "        print(f\"{subset.capitalize()} set: {total} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60740667-c70b-4042-91c0-bf5cb0276ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found labels: ['powdery_mildew', 'healthy']\n",
      "\n",
      "‚úÖ Dataset split complete.\n",
      "Train set: 2944 images\n",
      "Validation set: 840 images\n",
      "Test set: 424 images\n"
     ]
    }
   ],
   "source": [
    "organize_dataset_into_subsets(\n",
    "    image_dir=\"../inputs/cherry_leaves/cherry-leaves\",\n",
    "    output_dir=\"../inputs/split-leaves\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3a7a3-6456-4e0a-948b-a2625c2c0acf",
   "metadata": {},
   "source": [
    "#### ‚ÑπÔ∏è Note:\n",
    "A 70-20-10 split was chosen to ensure that the model has sufficient data to learn from (70%), while also providing enough examples to validate its performance during training (20%) and evaluate its generalization on unseen data (10%).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
